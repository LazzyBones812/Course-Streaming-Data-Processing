# Course-Streaming-Data-Processing
The project for the course "Streaming data processing"

**Цель проекта**: получать новостные данные о популярных ценных бумагах с финансового новостного агрегатора (в данном случае это Yahoo Finance), обрабатывать полученные данные и отрисовывать график в вебе. Передача данных осуществляется при помощи Kafka. 

**Библиотеки**
 - Scrapy - для получения данных с новостных источников
 - schedule - для того, чтобы паук Scrapy запускался через определенное время
 - confluent_kafka - для взаимодействия с Kafka
 - streamlit - для построение приложения в вебе
 - nltk - для обработки текстовых данных
 - plotly - для построения графиков

В *parse_fin_data\spiders* хранится файл yahoo_finance.py, в котором описана работа паука Scrapy. В этом файле в переменной lst хранится список тикетов ценных бумаг, информацию о которых паук получает. Данные передаются в топик kafka *some_topic*

В файле *parse_fin_data\main.py* описана работа schedule. Цель этой программы - запускать паука Scrapy и обработчик текстовых данных через каждые пять минут. Также тут создаются нужные топики Kafka и настроена конфигурация топика Kafka *res_topic* (retention.ms и segment.ms изменены так, чтобы данные из топика *res_topic* удалялись каждые пять минут - перед записью новых данных)

В файле *parse_fin_data\consumer_kafka.py* описан консьюмер kafka, который получает спарсенные с YahooFinance данные (с топика *some_topic*), преобразовывает их, подсчитывает частоту слов в полученных текстах и передает в топик *res_topic*

В файле *main_strm.py* данные приходят с топика *res_topic* и строится гистограмма 10-ти самых частовстречаемых слов в streamlit по адресу *localhost:8501* 

В файл results.json записываются промежуточные результаты, которые получил паук Scrapy

Для запуска приложения нужно ввести в терминале команду: docker compose -f docker-compose.yml up
